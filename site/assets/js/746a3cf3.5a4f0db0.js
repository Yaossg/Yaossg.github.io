"use strict";(self.webpackChunksite=self.webpackChunksite||[]).push([[6766],{8453:(n,e,t)=>{t.d(e,{R:()=>o,x:()=>a});var s=t(6540);const r={},i=s.createContext(r);function o(n){const e=s.useContext(i);return s.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:o(n.components),s.createElement(i.Provider,{value:e},n.children)}},9311:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>d,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>a,toc:()=>l});var s=t(4848),r=t(8453);const i={},o="\u264a \u53cc\u4eba\u6210\u884c",a={id:"cnss/6",title:"\u264a \u53cc\u4eba\u6210\u884c",description:"\u9898\u76ee",source:"@site/docs/cnss/6.md",sourceDirName:"cnss",slug:"/cnss/6",permalink:"/site/docs/cnss/6",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"\ud83d\udc0d \u770b\u770b\u86c7",permalink:"/site/docs/cnss/5"},next:{title:"\ud83c\udff4 \u81ea\u4e0b\u800c\u4e0a\u7684\u7a83\u542c\u653b\u51fb",permalink:"/site/docs/cnss/7"}},d={},l=[{value:"\u9898\u76ee",id:"\u9898\u76ee",level:2},{value:"\u9898\u89e3",id:"\u9898\u89e3",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,r.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.h1,{id:"-\u53cc\u4eba\u6210\u884c",children:"\u264a \u53cc\u4eba\u6210\u884c"}),"\n",(0,s.jsx)(e.h2,{id:"\u9898\u76ee",children:"\u9898\u76ee"}),"\n",(0,s.jsxs)(e.p,{children:["\u8bf7\u4f60\u8fd0\u7528\u6570\u636e\u5e76\u884c\u5316\uff0c\u4f7f\u7528 PyTorch \u7684 ",(0,s.jsx)(e.code,{children:"DistributedDataParallel"})," \u6a21\u7ec4\uff0c\u5b9e\u73b0\u4e00\u4e2a\u591a\u673a\u6216\u591a\u5361\u7684\u5bf9 ",(0,s.jsx)(e.code,{children:"CIFAR-10"})," \u8fdb\u884c\u5206\u7c7b\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u6a21\u578b\u81ea\u9009\u3002"]}),"\n",(0,s.jsx)(e.h2,{id:"\u9898\u89e3",children:"\u9898\u89e3"}),"\n",(0,s.jsx)(e.p,{children:"\u6ca1\u6709\u4e24\u4e2a GPU\uff0c\u7528 CPU \u548c GPU \u5e76\u884c\u51d1\u5408\u4e86\uff08"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import os\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.multiprocessing as mp\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\n\nfrom torch.nn.parallel import DistributedDataParallel\n\ndevices = [torch.device('cpu'), torch.device('cuda')]\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        )\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        )\n\n        self.dense = nn.Sequential(\n            nn.Linear(128, 120),\n            nn.ReLU(),\n            nn.Linear(120, 84),\n            nn.ReLU(),\n            nn.Linear(84, 10)\n        )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        x = x.view(-1, 128)\n        x = self.dense(x)\n        return x\n\n\ndef train(rank, world_size):\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n    torch.manual_seed(42)\n\n    device = devices[rank]\n    net = Net().to(device)\n    net = DistributedDataParallel(net)\n\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\n    trainset = torchvision.datasets.CIFAR10(root='./cifar-10', train=True, download=True,\n                                            transform=transforms.ToTensor())\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=5, shuffle=True)\n\n    CHECKPOINT = \"model.checkpoint.pkl\"\n\n    if rank == 0:\n        torch.save(net.state_dict(), CHECKPOINT)\n\n    dist.barrier()\n\n    net.load_state_dict(torch.load(CHECKPOINT, map_location=device))\n\n    for index, data in enumerate(trainloader):\n\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = net(inputs)\n        loss = criterion(outputs, labels).to(device)\n        loss.backward()\n        optimizer.step()\n\n    dist.barrier()\n\n    if rank == 0:\n        os.remove(CHECKPOINT)\n        torch.save(net, f'./model.pkl')\n\n    dist.destroy_process_group()\n\n\nif __name__ == '__main__':\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12345'\n    mp.freeze_support()\n    mp.spawn(train,\n             args=(len(devices), ),\n             nprocs=len(devices),\n             join=True)\n\n"})})]})}function p(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(c,{...n})}):c(n)}}}]);